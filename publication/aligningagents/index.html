<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.9.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.7eaca94f0cfe2f9115699dbdb8fbc775.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Adam Jelley"><meta name=description content="An investigation into training agents like Large Language Models (LLMs) by unsupervised pre-training, supervised fine-tuning, and finally reinforcement learning from human feedback (RLHF)."><link rel=alternate hreflang=en-us href=https://adamjelley.github.io/publication/aligningagents/><link rel=canonical href=https://adamjelley.github.io/publication/aligningagents/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hudb77e0d6599affbdab246cf9ea514ec9_35945_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hudb77e0d6599affbdab246cf9ea514ec9_35945_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:image" content="https://adamjelley.github.io/publication/aligningagents/featured.png"><meta property="og:type" content="article"><meta property="og:site_name" content="Adam Jelley"><meta property="og:url" content="https://adamjelley.github.io/publication/aligningagents/"><meta property="og:title" content="Aligning Agents like Large Language Models | Adam Jelley"><meta property="og:description" content="An investigation into training agents like Large Language Models (LLMs) by unsupervised pre-training, supervised fine-tuning, and finally reinforcement learning from human feedback (RLHF)."><meta property="og:image" content="https://adamjelley.github.io/publication/aligningagents/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2017-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-09T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://adamjelley.github.io/publication/aligningagents/"},"headline":"Aligning Agents like Large Language Models","image":["https://adamjelley.github.io/publication/aligningagents/featured.png"],"datePublished":"2017-01-01T00:00:00Z","dateModified":"2024-08-09T00:00:00Z","author":{"@type":"Person","name":"Adam Jelley"},"publisher":{"@type":"Organization","name":"Adam Jelley","logo":{"@type":"ImageObject","url":"https://adamjelley.github.io/media/icon_hudb77e0d6599affbdab246cf9ea514ec9_35945_192x192_fill_lanczos_center_3.png"}},"description":"An investigation into training agents like Large Language Models (LLMs) by unsupervised pre-training, supervised fine-tuning, and finally reinforcement learning from human feedback (RLHF)."}</script><title>Aligning Agents like Large Language Models | Adam Jelley</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=ae9ed045335921aee68133484ad9f6ed><script src=/js/wowchemy-init.min.980187dd755ac492a5ed9f5d2ea310ab.js></script><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Adam Jelley</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Adam Jelley</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=/uploads/CV.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Aligning Agents like Large Language Models</h1><div class=article-metadata><div><span class=author-highlighted>Adam Jelley</span>, <span>Yuhan Cao</span>, <span>Dave Bignell</span>, <span>Sam Devlin</span>, <span>Tabish Rashid</span></div><span class=article-date>August 2024</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2406.04208 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/aligningagents/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://adamjelley.github.io/aligning-agents-like-llms/ target=_blank rel=noopener>Project</a>
<a class="btn btn-outline-primary btn-page-header" href=https://doi.org/https://doi.org/10.48550/arXiv.2406.04208 target=_blank rel=noopener>DOI</a></div></div><div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:720px;max-height:1019px><div style=position:relative><img src=/publication/aligningagents/featured_hu687779cfe35da4aa92a8751b3222f633_9135847_2cd9a4d2b7d7d580093adb2b2f0dd6b7.webp width=720 height=1019 alt class=featured-image></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Training agents to behave as desired in complex 3D environments from high-dimensional sensory information is challenging. Imitation learning from diverse human behavior provides a scalable approach for training an agent with a sensible behavioral prior, but such an agent may not perform the specific behaviors of interest when deployed. To address this issue, we draw an analogy between the undesirable behaviors of imitation learning agents and the unhelpful responses of unaligned large language models (LLMs). We then investigate how the procedure for aligning LLMs can be applied to aligning agents in a 3D environment from pixels. For our analysis, we utilize an academically illustrative part of a modern console game in which the human behavior distribution is multi-modal, but we want our agent to imitate a single mode of this behavior. We demonstrate that we can align our agent to consistently perform the desired mode, while providing insights and advice for successfully applying this approach to training agents. Project webpage at <a href=https://adamjelley.github.io/aligning-agents-like-llms/ target=_blank rel=noopener>https://adamjelley.github.io/aligning-agents-like-llms/</a>.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>1</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Workshop on Reinforcement Learning Beyond Rewards at the Reinforcement Learning Conference (RLC) 2024</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fadamjelley.github.io%2Fpublication%2Faligningagents%2F&amp;text=Aligning+Agents+like+Large+Language+Models" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fadamjelley.github.io%2Fpublication%2Faligningagents%2F&amp;t=Aligning+Agents+like+Large+Language+Models" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Aligning%20Agents%20like%20Large%20Language%20Models&amp;body=https%3A%2F%2Fadamjelley.github.io%2Fpublication%2Faligningagents%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fadamjelley.github.io%2Fpublication%2Faligningagents%2F&amp;title=Aligning+Agents+like+Large+Language+Models" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://adamjelley.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hua3b2844ab5470f518bf3e012e3758368_1400082_270x270_fill_q75_lanczos_center.jpg alt="Adam Jelley"></a><div class=media-body><h5 class=card-title><a href=https://adamjelley.github.io/>Adam Jelley</a></h5><h6 class=card-subtitle>PhD Student in Deep Reinforcement Learning</h6><p class=card-text>PhD Student in efficient reinforcement learning.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:adam.jelley@ed.ac.uk><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=39t3yJcAAAAJ" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/AdamJelley target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/adamjelley target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.72a17850a2ab0825ad2ca80ba3652d55.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js type=module></script></body></html>